{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filePath = \"~/RecSys/ratings_Musical_Instruments.csv\"\n",
    "df = pd.read_csv(filePath,delimiter=\",\")\n",
    "\n",
    "df['tid'] = np.arange(len(df))\n",
    "\n",
    "userMappings = dict(zip(df.user_id.unique(),range(len(df.user_id.unique()))))\n",
    "userRevMapping = {userId:userIdx for (userIdx,userId) in userMappings.items()}\n",
    "\n",
    "df['user_idx'] = df.user_id.apply(lambda x: userMappings[x])\n",
    "\n",
    "itemMappings = dict(zip(df.item_id.unique(),range(len(df.item_id.unique()))))\n",
    "itemRevMapping = {itemId:itemIdx for (itemIdx,itemId) in itemMappings.items()}\n",
    "df['item_idx'] = df.item_id.apply(lambda x: itemMappings[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 395270\n",
      "Size of test set: 104906\n",
      "Ratio: 79/21\n"
     ]
    }
   ],
   "source": [
    "itemGroups = df.groupby('item_id')\n",
    "noRatingsPerItem = itemGroups.apply(len)\n",
    "userGroups = df.groupby('user_id')\n",
    "noRatingsPerUser = userGroups.apply(len)\n",
    "                                    \n",
    "def sampleFrom(x):\n",
    "    #p = noRatingsPerUser.loc[x.user_id]\n",
    "    #p = p / p.sum()\n",
    "    size = int(math.ceil(0.5*len(x)))\n",
    "    return x.iloc[np.random.choice(range(len(x)),size=size)]\n",
    "trainData = df.groupby('user_id').apply(sampleFrom)\n",
    "trainData = df[df.tid.isin(trainData.tid.values) == True]\n",
    "testData = df[df.tid.isin(trainData.tid.values) == False]\n",
    "\n",
    "temp = testData[testData.item_id.isin(trainData.item_id.values) == False]\n",
    "testData = testData[testData.item_id.isin(trainData.item_id.values) == True]\n",
    "trainData = trainData.append(temp)\n",
    "\n",
    "print(\"Size of training set: {0}\".format(len(trainData)))\n",
    "print(\"Size of test set: {0}\".format(len(testData)))\n",
    "print(\"Ratio: {0}/{1}\".format(int(100 * float(len(trainData)) / len(df)),\n",
    "                              int(math.ceil(100 * float(len(testData)) / len(df)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creating tensorflow network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "mu = tf.Variable(tf.zeros(1),name='global_bias',dtype=tf.float32)\n",
    "\n",
    "noUsers = len(df.user_id.unique())\n",
    "userBias = tf.Variable(tf.random_uniform([noUsers],minval=0,maxval=1),name='user_bias',dtype=tf.float32)\n",
    "\n",
    "noItems = len(df.item_id.unique())\n",
    "itemBias = tf.Variable(tf.random_uniform([noItems],minval=0,maxval=1),name='item_bias',dtype=tf.float32)\n",
    "\n",
    "latentFactors = 5\n",
    "userFactors = tf.Variable(tf.random_uniform([noUsers, latentFactors],minval=0,maxval=1),\n",
    "                          name='user_factors',dtype=tf.float32)\n",
    "itemFactors = tf.Variable(tf.random_uniform([noItems, latentFactors],minval=0,maxval=1),\n",
    "                          name='item_factors',dtype=tf.float32)\n",
    "\n",
    "userId = tf.placeholder(tf.int32, [None], name='user_id')\n",
    "itemId = tf.placeholder(tf.int32, [None], name='item_id')\n",
    "\n",
    "userBiasLU = tf.nn.embedding_lookup(userBias, userId)\n",
    "userFactorLU = tf.nn.embedding_lookup(userFactors, userId)\n",
    "\n",
    "itemBiasLU = tf.nn.embedding_lookup(itemBias, itemId)\n",
    "itemFactorLU = tf.nn.embedding_lookup(itemFactors, itemId)\n",
    "\n",
    "predRatings = mu + itemBiasLU + userBiasLU + tf.reduce_sum(tf.multiply(itemFactorLU,userFactorLU),axis=1)\n",
    "topKItems = tf.nn.top_k(predRatings,k=10,name='recommendations')\n",
    "actRatings = tf.placeholder(tf.float32,[None],name='actual_ratings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squaredLoss = tf.losses.mean_squared_error(actRatings,predRatings)\n",
    "rmse = tf.sqrt(squaredLoss)\n",
    "maeLoss = tf.reduce_mean(tf.abs(actRatings - predRatings))\n",
    "\n",
    "userReg = tf.nn.l2_loss(userFactors)\n",
    "itemReg = tf.nn.l2_loss(itemFactors)\n",
    "\n",
    "beta = 0.5\n",
    "loss = maeLoss + beta*userReg + beta*itemReg\n",
    "train_op = tf.contrib.layers.optimize_loss(loss=loss,\n",
    "                                           global_step=tf.contrib.framework.get_global_step(),\n",
    "                                           learning_rate=0.001,\n",
    "                                           optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n",
      "\t Training Loss: 101423.082427\n",
      "\n",
      "\n",
      "\t Test RMSE Loss: 2.82385182381\n",
      "\t      MAE Loss: 2.6483271122\n",
      "Episode:  10\n",
      "\t Training Loss: 0.799183349217\n",
      "\n",
      "\n",
      "\t Test RMSE Loss: 1.11031866074\n",
      "\t      MAE Loss: 0.776543736458\n",
      "Episode:  20\n",
      "\t Training Loss: 0.590988835353\n",
      "\n",
      "\n",
      "\t Test RMSE Loss: 1.08190143108\n",
      "\t      MAE Loss: 0.72132652998\n",
      "Episode:  30\n",
      "\t Training Loss: 0.464806470313\n",
      "\n",
      "\n",
      "\t Test RMSE Loss: 1.07629358768\n",
      "\t      MAE Loss: 0.715110480785\n",
      "Episode:  40\n",
      "\t Training Loss: 0.376561033047\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "fetches = {'eval_op':train_op}\n",
    "noEpisodes = 100\n",
    "batchSize = 1000\n",
    "noEpochs = len(trainData) / batchSize\n",
    "\n",
    "testFeed = {userId:testData.user_idx.values,\n",
    "            itemId:testData.item_idx.values,\n",
    "            actRatings:testData.rating.values}\n",
    "\n",
    "prevMAE = 10000\n",
    "prevRMSE = 10000\n",
    "\n",
    "for episode in range(noEpisodes):\n",
    "    episodeData = trainData.iloc[np.random.permutation(len(trainData))]\n",
    "    startIdx = 0\n",
    "    episodeLoss = 0\n",
    "    for epoch in range(noEpochs):\n",
    "        batch = episodeData.iloc[startIdx:startIdx+batchSize]\n",
    "        startIdx += batchSize\n",
    "\n",
    "        labels = batch.rating.values\n",
    "        userIds = batch.user_idx.values\n",
    "        itemIds = batch.item_idx.values\n",
    "        feed_dict = {userId:userIds,itemId:itemIds,actRatings:labels}\n",
    "        trainMetrics = sess.run(fetches,feed_dict)\n",
    "        episodeLoss += trainMetrics['eval_op']\n",
    "\n",
    "    episodeLoss /= noEpochs\n",
    "\n",
    "    if(episode % 10 == 0):\n",
    "        print \"Episode: \",episode\n",
    "        print \"\\t Training Loss: {0}\".format(episodeLoss)\n",
    "        print \"\\n\"\n",
    "\n",
    "        fetches = {'rmse':rmse,'mae':maeLoss}\n",
    "        testMetrics = sess.run(fetches,testFeed)\n",
    "        \n",
    "        print \"\\t Test RMSE Loss: {0}\".format(testMetrics['rmse'])\n",
    "        print \"\\t      MAE Loss: {0}\".format(testMetrics['mae'])\n",
    "        \n",
    "        if(prevRMSE < testMetrics['rmse'] or prevMAE < testMetrics['mae']):\n",
    "            break\n",
    "        \n",
    "        prevRMSE = testMetrics['rmse']\n",
    "        prevMAE = testMetrics['mae']\n",
    "        \n",
    "        fetches = {'eval_op':train_op}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = 0\n",
    "testItems = np.arange(noItems) #testData.item_idx.unique()\n",
    "noItemsInTest = len(testItems)\n",
    "fetches = {'topK':topKItems,'predRatings':predRatings}\n",
    "for uid in range(250):\n",
    "    userFeed = {userId:np.ones((noItemsInTest)) * uid,\n",
    "                itemId:testItems}\n",
    "    retVal = sess.run(fetches=fetches,feed_dict=userFeed)\n",
    "    topKRecommendations = retVal['topK']\n",
    "    topKIds = testItems[topKRecommendations.indices]\n",
    "    x = userGroups.get_group(userRevMapping[uid])\n",
    "    lTemp = len(set(x.item_idx.values) & set(topKIds))\n",
    "    if(l < lTemp):\n",
    "        l = lTemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.31017876,  5.3075633 ,  5.30155897,  5.30086851,  5.30084991,\n",
       "        5.2982378 ,  5.28893566,  5.2855463 ,  5.27991915,  5.27857733], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = np.sort(retVal['predRatings'])[::-1]\n",
    "temp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259          32\n",
       "146367    20561\n",
       "164839    23258\n",
       "Name: item_idx, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.31017876,  5.3075633 ,  5.30155897,  5.30086851,  5.30084991,\n",
       "        5.2982378 ,  5.28893566,  5.2855463 ,  5.27991915,  5.27857733], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topKRecommendations.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9693661"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retVal['predRatings'][20561]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
