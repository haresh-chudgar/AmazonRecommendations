{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data from original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filePath = \"~/RecSys/ratings_Musical_Instruments.csv\"\n",
    "df = pd.read_csv(filePath,delimiter=\",\")\n",
    "\n",
    "df['tid'] = np.arange(len(df))\n",
    "\n",
    "userMappings = dict(zip(df.user_id.unique(),range(len(df.user_id.unique()))))\n",
    "userRevMapping = {userId:userIdx for (userIdx,userId) in userMappings.items()}\n",
    "\n",
    "df['user_idx'] = df.user_id.apply(lambda x: userMappings[x])\n",
    "\n",
    "itemMappings = dict(zip(df.item_id.unique(),range(len(df.item_id.unique()))))\n",
    "itemRevMapping = {itemId:itemIdx for (itemIdx,itemId) in itemMappings.items()}\n",
    "df['item_idx'] = df.item_id.apply(lambda x: itemMappings[x])\n",
    "\n",
    "itemGroups = df.groupby('item_id')\n",
    "noRatingsPerItem = itemGroups.apply(len)\n",
    "userGroups = df.groupby('user_id')\n",
    "noRatingsPerUser = userGroups.apply(len)\n",
    "\n",
    "#Creating train and test sets\n",
    "\n",
    "def sampleFrom(x):\n",
    "    #p = noRatingsPerUser.loc[x.user_id]\n",
    "    #p = p / p.sum()\n",
    "    size = int(math.ceil(0.5*len(x)))\n",
    "    return x.iloc[np.random.choice(range(len(x)),size=size)]\n",
    "trainData = df.groupby('user_id').apply(sampleFrom)\n",
    "trainData = df[df.tid.isin(trainData.tid.values) == True]\n",
    "testData = df[df.tid.isin(trainData.tid.values) == False]\n",
    "\n",
    "temp = testData[testData.item_id.isin(trainData.item_id.values) == False]\n",
    "testData = testData[testData.item_id.isin(trainData.item_id.values) == True]\n",
    "trainData = trainData.append(temp)\n",
    "\n",
    "print(\"Size of training set: {0}\".format(len(trainData)))\n",
    "print(\"Size of test set: {0}\".format(len(testData)))\n",
    "print(\"Ratio: {0}/{1}\".format(int(100 * float(len(trainData)) / len(df)),\n",
    "                              int(math.ceil(100 * float(len(testData)) / len(df)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load from saved files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filePath = \"~/RecSys/ratings_Musical_Instruments.csv\"\n",
    "df = pd.read_csv(filePath,delimiter=\",\")\n",
    "userGroups = df.groupby('user_id')\n",
    "\n",
    "trainData = pd.read_csv('/home/ubuntu/RecSys/trainData.csv')\n",
    "testData = pd.read_csv('/home/ubuntu/RecSys/testData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userMappings = dict(zip(df.user_id.unique(),range(len(df.user_id.unique()))))\n",
    "userRevMapping = {userId:userIdx for (userIdx,userId) in userMappings.items()}\n",
    "\n",
    "df['user_idx'] = df.user_id.apply(lambda x: userMappings[x])\n",
    "\n",
    "itemMappings = dict(zip(df.item_id.unique(),range(len(df.item_id.unique()))))\n",
    "itemRevMapping = {itemId:itemIdx for (itemIdx,itemId) in itemMappings.items()}\n",
    "df['item_idx'] = df.item_id.apply(lambda x: itemMappings[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creating tensorflow network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "mu = tf.Variable(tf.zeros(1),name='global_bias',dtype=tf.float32)\n",
    "\n",
    "noUsers = len(df.user_id.unique())\n",
    "userBias = tf.Variable(tf.random_uniform([noUsers],minval=0,maxval=1),name='user_bias',dtype=tf.float32)\n",
    "\n",
    "noItems = len(df.item_id.unique())\n",
    "itemBias = tf.Variable(tf.random_uniform([noItems],minval=0,maxval=1),name='item_bias',dtype=tf.float32)\n",
    "\n",
    "latentFactors = 50\n",
    "userFactors = tf.Variable(tf.random_uniform([noUsers, latentFactors],minval=0,maxval=1),\n",
    "                          name='user_factors',dtype=tf.float32)\n",
    "itemFactors = tf.Variable(tf.random_uniform([noItems, latentFactors],minval=0,maxval=1),\n",
    "                          name='item_factors',dtype=tf.float32)\n",
    "\n",
    "userId = tf.placeholder(tf.int32, [None], name='user_id')\n",
    "itemId = tf.placeholder(tf.int32, [None], name='item_id')\n",
    "\n",
    "userBiasLU = tf.nn.embedding_lookup(userBias, userId)\n",
    "userFactorLU = tf.nn.embedding_lookup(userFactors, userId)\n",
    "\n",
    "itemBiasLU = tf.nn.embedding_lookup(itemBias, itemId)\n",
    "itemFactorLU = tf.nn.embedding_lookup(itemFactors, itemId)\n",
    "\n",
    "predRatings = mu + itemBiasLU + userBiasLU + tf.reduce_sum(tf.multiply(itemFactorLU,userFactorLU),axis=1)\n",
    "topKItems = tf.nn.top_k(predRatings,k=10,name='recommendations')\n",
    "actRatings = tf.placeholder(tf.float32,[None],name='actual_ratings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squaredLoss = tf.losses.mean_squared_error(actRatings,predRatings)\n",
    "rmse = tf.sqrt(squaredLoss)\n",
    "maeLoss = tf.reduce_mean(tf.abs(actRatings - predRatings))\n",
    "\n",
    "userReg = tf.reduce_mean(tf.square(userFactorLU))\n",
    "itemReg = tf.reduce_mean(tf.square(itemFactorLU))\n",
    "\n",
    "beta = 0.05\n",
    "loss = maeLoss + beta*userReg + beta*itemReg\n",
    "train_op = tf.contrib.layers.optimize_loss(loss=loss,\n",
    "                                           global_step=tf.contrib.framework.get_global_step(),\n",
    "                                           learning_rate=0.01,\n",
    "                                           optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode no, train loss, test rmse, test mae\n",
      "0,3.6383128502,2.20157527924,1.75790774822\n",
      "1,1.74262724492,2.14853167534,1.6839094162\n",
      "2,1.71459503464,2.03091835976,1.60903608799\n",
      "3,1.49034933632,1.95286226273,1.54688000679\n",
      "4,1.37099198399,1.81477129459,1.44539916515\n",
      "5,1.21037630049,1.72896134853,1.37855124474\n",
      "6,1.11121500718,1.62254393101,1.29827749729\n",
      "7,1.00514532542,1.55774867535,1.24690651894\n",
      "8,0.948956213173,1.47882175446,1.18306171894\n",
      "9,0.870989158674,1.43192255497,1.14648580551\n",
      "10,0.84964560948,1.37449169159,1.09818899632\n",
      "11,0.789433303521,1.33490741253,1.05992686749\n",
      "12,0.7816856682,1.29897844791,1.0244781971\n",
      "13,0.737846340927,1.27742052078,1.00519573689\n",
      "14,0.735366341578,1.2526216507,0.9807690382\n",
      "15,0.7017516872,1.23736250401,0.963768005371\n",
      "16,0.699468786946,1.21871197224,0.94155395031\n",
      "17,0.67787554497,1.20814108849,0.926775693893\n",
      "18,0.673996503734,1.20088040829,0.916720569134\n",
      "19,0.656768002335,1.18900370598,0.898796081543\n",
      "20,0.653934389504,1.19100415707,0.90294867754\n",
      "21,0.641348244424,1.18559408188,0.890924632549\n",
      "22,0.637118741946,1.18524610996,0.890342772007\n",
      "23,0.627192315414,1.1808617115,0.878838956356\n",
      "24,0.621301453504,1.18200564384,0.881600558758\n",
      "25,0.616271634392,1.18607342243,0.880782544613\n",
      "26,0.607923084709,1.18669128418,0.881318092346\n",
      "27,0.607317844245,1.18378996849,0.873346030712\n",
      "28,0.59642884052,1.18696022034,0.877528071404\n",
      "29,0.599441703957,1.18708372116,0.874400734901\n",
      "30,0.587285521672,1.18823003769,0.871686756611\n",
      "31,0.593307203977,1.19307422638,0.877133905888\n",
      "32,0.580854467556,1.1949262619,0.874347031116\n",
      "33,0.58475496064,1.19631004333,0.875452160835\n",
      "34,0.574873644461,1.19811987877,0.873472630978\n",
      "35,0.576930730627,1.19972121716,0.876516103745\n",
      "36,0.572544169381,1.20188760757,0.878428459167\n",
      "37,0.570845199418,1.20716011524,0.881676077843\n",
      "38,0.567692473455,1.20545780659,0.877800524235\n",
      "39,0.566700138719,1.2100032568,0.881205320358\n",
      "40,0.563648688839,1.20891129971,0.878524184227\n",
      "41,0.561137639856,1.21130931377,0.880582094193\n",
      "42,0.561803986881,1.21427762508,0.882709145546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-f8ea406e8101>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mitemIds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0muserId\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0muserIds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitemId\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mitemIds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactRatings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtrainMetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mepisodeLoss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrainMetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eval_op'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "fetches = {'eval_op':train_op}\n",
    "noEpisodes = 100\n",
    "batchSize = 1000\n",
    "noEpochs = len(trainData) / batchSize\n",
    "\n",
    "testFeed = {userId:testData.user_idx.values,\n",
    "            itemId:testData.item_idx.values,\n",
    "            actRatings:testData.rating.values}\n",
    "\n",
    "prevMAE = 10000\n",
    "prevRMSE = 10000\n",
    "\n",
    "print(\"episode no, train loss, test rmse, test mae\")\n",
    "for episode in range(noEpisodes):\n",
    "    episodeData = trainData.iloc[np.random.permutation(len(trainData))]\n",
    "    startIdx = 0\n",
    "    episodeLoss = 0\n",
    "    for epoch in range(noEpochs):\n",
    "        batch = episodeData.iloc[startIdx:startIdx+batchSize]\n",
    "        startIdx += batchSize\n",
    "\n",
    "        labels = batch.rating.values\n",
    "        userIds = batch.user_idx.values\n",
    "        itemIds = batch.item_idx.values\n",
    "        feed_dict = {userId:userIds,itemId:itemIds,actRatings:labels}\n",
    "        trainMetrics = sess.run(fetches,feed_dict)\n",
    "        episodeLoss += trainMetrics['eval_op']\n",
    "\n",
    "    episodeLoss /= noEpochs\n",
    "\n",
    "    fetches = {'rmse':rmse,'mae':maeLoss}\n",
    "    testMetrics = sess.run(fetches,testFeed)\n",
    "\n",
    "    print \"{},{},{},{}\".format(episode,episodeLoss,testMetrics['rmse'],testMetrics['mae'])\n",
    "        \n",
    "    prevRMSE = testMetrics['rmse']\n",
    "    prevMAE = testMetrics['mae']\n",
    "\n",
    "    fetches = {'eval_op':train_op}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0305986186404\n"
     ]
    }
   ],
   "source": [
    "precisionAtK = 0\n",
    "fetches = {'topK':topKItems,'predRatings':predRatings}\n",
    "for uid in range(noUsers):\n",
    "    testItems = np.random.randint(noItems,size=(1000))\n",
    "    x = userGroups.get_group(userRevMapping[0])\n",
    "    ratedItems = x.item_idx.unique()\n",
    "    testItems = np.append(testItems,ratedItems)\n",
    "    testItems = np.array(list(set(testItems)),dtype=np.int32)\n",
    "    userFeed = {userId:np.ones((len(testItems))) * uid,\n",
    "                itemId:testItems}\n",
    "    retVal = sess.run(fetches=fetches,feed_dict=userFeed)\n",
    "    topKRecommendations = retVal['topK']\n",
    "    topKIds = testItems[topKRecommendations.indices]\n",
    "    precisionAtK += len(set(ratedItems) & set(topKIds))\n",
    "\n",
    "precisionAtK = 100.0 * precisionAtK / (10.0 * noUsers)\n",
    "print(\"Precision@10: {0}\".format(precisionAtK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = trainData[['user_idx','item_idx','rating']]\n",
    "#temp.to_csv('/home/ubuntu/RecSys/trainRatings.csv')\n",
    "f = open('/home/ubuntu/RecSys/trainRatings.csv','w')\n",
    "np.savetxt(f,temp.values,fmt='%d',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = testData[['user_idx','item_idx','rating']]\n",
    "f = open('/home/ubuntu/RecSys/testRatings.csv','w')\n",
    "np.savetxt(f,temp.values,fmt='%d',delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
